{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnoVando/MSIS/blob/master/MSIS521_IA2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTIHcVguF5qI",
        "colab_type": "code",
        "outputId": "a2d8707a-78b8-4cb6-bdb4-eaf0b663a7f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt') # tokenizer\n",
        "nltk.download('wordnet') # lemmatizer\n",
        "nltk.download('stopwords') # used to handle words like a, an, the\n",
        "nltk.download('averaged_perceptron_tagger') # Part of Speech\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcT7WHOlF56J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://github.com/AnoVando/MSIS/raw/master/IA2.csv'\n",
        "data = pd.read_csv(url, header=None)\n",
        "data.columns = ['index', 'comment']\n",
        "rows = data['comment'].tolist() # store comments as a list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2B_TX__F-TN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "comments = [] # list to store all tokens, one token per row\n",
        "\n",
        "# iterate through the list and tokenize each comment\n",
        "for row in rows:\n",
        "  comments.append(nltk.word_tokenize(row))\n",
        "\n",
        "whitespace = nltk.tokenize.WhitespaceTokenizer() # generate whitespace function\n",
        "\n",
        "tokens = [] # create list to store the whitespace tokenized values\n",
        "\n",
        "# iterate through the rows and apply the whitespace function\n",
        "for row in rows:\n",
        "  tokens = whitespace.tokenize(row)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvgM8DxLY1-A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5187909d-f078-4ffe-d9d9-fd1933243092"
      },
      "source": [
        "print(tokens)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Some', 'of', 'Liu', 'Zhenyun’s', 'works', 'have', 'been', 'read', 'intermittently.', 'The', 'small', 'forest', 'in', 'the', '“Unit”', 'and', '“Chicken', 'of', 'the', 'Land”', 'grew', 'up', 'under', 'the', 'reform', 'and', 'opening', 'up', 'and', 'the', 'market', 'economy.', 'Many', 'historical', 'figures', 'in', 'the', '“Hometown”', 'series', 'encircle', 'the', 'historical', 'reality,', 'nothing', 'more', 'than', 'Liu', 'Zhenyun’s', 'black', 'humor.', 'The', 'language', 'reveals', 'his', 'embarrassment', 'as', 'a', 'writer.', 'The', 'characters', 'are', 'not', 'important.', 'What', 'is', 'important', 'is', 'to', 'dispel', 'the', 'grand', 'narrative', 'in', 'the', 'historical', 'context,', 'to', 'defeat', 'the', 'tall', 'and', 'complete,', 'and', 'to', 'return', 'a', 'real', 'name', 'to', 'the', 'world.', 'In', 'Liu', 'Zhenyun&#39;s', 'works,', 'you', 'can', 'almost', 'see', 'too', 'many', 'postmodernist', 'techniques,', 'rather', 'than', 'the', 'pure', 'modernity', 'of', 'the', 'sorrowful,', 'magical', 'realism,', 'in', 'the', '&quot;hometown', 'circulate&quot;,', 'the', 'six', 'fingers', 'of', 'the', 'excess', 'fingers', 'set', 'up', 'a', 'bridge,', 'let', 'Hundreds', 'of', 'thousands', 'of', 'immigrant', 'Yanjin', 'people', 'have', 'successfully', 'crossed', 'the', 'Yellow', 'River.', 'In', 'the', 'new', 'historicalism,', 'in', 'the', '&quot;Hometown', 'of', 'the', 'Yellow', 'Flower&quot;,', 'the', 'village', 'committee', 'cadres', 'are', 'not', 'headed', 'for', 'the', 'people', 'like', 'the', 'textbooks', 'of', 'history.', 'Instead,', 'they', 'ride', 'on', 'the', 'people&#39;s', 'heads', 'to', 'make', 'a', 'fortune', 'and', 'fight.', 'Through', 'the', 'ubiquity', 'of', 'historical', 'figures', 'in', '&quot;The', 'Story', 'of', 'Hometown&quot;,', 'Liu', 'Zhenyun', 'also', 'broke', 'the', 'traditional', 'narrative,', 'and', 'the', 'meta-fictional', 'narrative', 'technique', 'has', 'stirred', 'up', 'the', 'history.', 'At', 'the', 'end', 'of', 'the', 'day,', 'it’s', 'a', 'slap', 'in', 'the', 'word,', 'it’s', 'written', 'in', 'a', 'fascinating', 'way,', 'and', 'it’s', 'nothing', 'more', 'than', 'a', 'slap', 'in', 'the', 'face.', 'It’s', 'nothing', 'more', 'than', 'a', 'ready-made', 'material.', 'It’s', 'nothing', 'more', 'than', 'an', 'attempt', 'to', 'dispel', 'the', 'real,', 'blinded', 'review.', 'Writing', 'strategies,', 'in', 'terms', 'of', 'materials,', 'are', 'neither', 'beneficial', 'nor', 'serious.', 'Everyone', 'can', 'write', 'novels,', 'but', 'not', 'everyone', 'is', 'a', 'novelist,', 'and', 'not', 'everyone', 'is', 'a', 'writer.', 'Mr.', 'Liu', 'Zhenyun', 'is', 'more', 'willing', 'to', 'put', 'it', 'between', 'the', 'novelist', 'and', 'the', 'writer.', 'Once', 'the', 'language', 'of', 'Liu', 'Zhenyun&#39;s', 'novels', 'is', 'heard,', 'everyone', 'can', 'imitate', 'it.', 'Secondly,', 'Liu', 'Zhenyun&#39;s', 'delicate', 'observation', 'is', 'unparalleled.', 'In', '&quot;Wen', 'Wen', '1942&quot;,', 'it', 'is', 'to', 'use', 'data', 'to', 'speak,', 'half', 'is', 'like', 'late', 'reportage,', 'and', 'in', '&quot;Unit&quot;,', '&quot;One', 'Place', 'Chicken&quot;,', 'the', 'language', 'of', 'the', 'new', 'realist', 'writer', 'is', 'infinitely', 'close', 'to', 'real', 'life,', 'unlimited', 'The', 'degree', 'of', 'closeness', 'to', 'the', 'real', 'life', 'of', 'rice,', 'oil', 'and', 'salt,', 'unlimited', 'feelings', 'helpless,', 'and', 'even', 'a', 'little', 'gloating.', 'In', 'the', '&quot;Hometown&quot;', 'series,', 'this', 'language', 'has', 'flooded', 'to', 'the', 'extreme,', 'and', 'the', 'fart', 'is', 'flying,', 'and', 'the', 'spit', 'is', 'smeared.', 'Although', 'it', 'likes', 'the', 'extreme,', 'the', 'language&#39;s', 'popularization', 'restricts', 'the', 'theme', 'to', 'a', 'deeper', 'level.', 'The', 'biggest', 'characteristic', 'of', 'Liu', 'Zhenyun', 'is', 'that', 'he', 'can', 'go', 'freely', 'in', 'the', 'switching', 'between', 'the', 'microscope', 'and', 'the', 'magnifying', 'glass.', 'His', 'microscope', 'is', 'generally', 'meticulously', 'observing', 'life.', 'In', 'the', '&quot;Unit&quot;', '&quot;The', 'Chicken', 'in', 'a', 'Field&quot;,', 'he', 'is', 'alert', 'to', 'the', 'possibility', 'of', 'a', 'loose', 'political', 'atmosphere,', 'and', 'the', 'fledgling', 'The', 'growth', 'of', 'Kobayashi', 'is', 'a', 'true', 'portrayal', 'of', 'the', 'history', 'of', 'this', 'loose', 'atmosphere.', 'If', 'a', 'person', 'wants', 'to', 'grow,', 'he', 'must', 'be', 'washed', 'by', 'the', 'unit.Practice', 'can', 'only', 'succeed.', 'In', 'the', '&quot;Hometown&quot;', 'series,', 'he', 'keenly', 'wrote', 'that', 'the', 'subtle', 'relationship', 'between', 'people', 'changes', 'the', 'existence', 'of', 'the', 'hidden', 'class', 'with', 'the', 'difference', 'of', 'economic', 'status', 'and', 'power', 'possession,', 'and', 'the', 'story', 'of', '&quot;The', 'Hometown', 'Gossip&quot;', 'from', 'the', 'Three', 'Kingdoms', 'Period', 'The', 'bear-like', 'look', 'turned', 'into', 'a', 'heroic', 'village', 'party', 'secretary', 'for', '59-61', 'years.', 'The', 'different', 'attitudes', 'of', 'the', 'dogs', 'are', 'the', 'proof.', 'During', 'the', 'great', 'migration', 'period', 'of', 'the', 'Ming', 'Dynasty,', 'the', 'people’s', 'return', 'to', 'the', 'six-finger', 'attitude', 'was', 'also', 'supported.', 'Liu', 'Zhenyun’s', 'magnifying', 'glass', 'tells', 'a', 'weak,', 'humble,', 'bullying', 'and', 'hard-working', 'nation', 'through', 'these', 'small', 'things', 'that', 'seem', 'to', 'be', 'overwhelmed', 'by', 'historical', 'floods.', 'It', 'has', 'always', 'been', 'used', 'in', 'power', 'changes', 'for', 'thousands', 'of', 'years.', 'In', '&quot;Hometown', 'of', 'the', 'Yellow', 'Flower&quot;,', 'large-scale', 'slogans', 'and', 'policies', 'were', 'replaced', 'by', 'the', 'game', 'of', 'power.', 'The', 'power', 'given', 'by', 'the', 'people', 'to', 'be', 'the', 'master', 'of', 'the', 'house', 'was', 'swallowed', 'up', 'by', 'flattery,', 'selfishness,', 'and', 'numbness,', 'for', 'thousands', 'of', 'years.', 'I', 'remember', 'listening', 'to', 'Liu', 'Zhenyun’s', 'speech', 'once.', 'He', 'said', 'that', 'he', 'wanted', 'to', 'write', 'the', 'old', 'forest', '30', 'years', 'later.', 'Xiaolin', 'was', '30', 'years', 'ago.', 'The', 'old', 'forest', 'is', '30', 'years', 'later,', 'and', 'people', 'may', 'not.', 'He', 'perceives', 'changes', 'in', 'social', 'trends,', 'and', 'we', 'will', 'not', 'be', 'remembered', 'by', 'history.', 'However,', 'changes', 'in', 'history', 'and', 'society', 'will', 'bring', 'about', 'changes', 'in', 'the', 'principles', 'of', 'affairs', 'and', 'interpersonal', 'relationships.', 'In', 'the', 'context', 'of', 'small', 'people,', 'it', 'is', 'a', 'pity', 'that', 'genes', 'are', 'not', 'necessarily.', 'Got', 'the', 'money.', 'In', 'a', 'carnival-like', 'discourse', 'magnetic', 'field', 'made', 'up', 'of', 'rough', 'language,', 'it', 'is', 'enchanting', 'and', 'fascinating.', 'I', 'waited', 'for', 'the', 'heavenly', 'ass,', 'the', 'dance', 'of', 'the', 'natural', 'hand.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B86bWymlTQP0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "be099260-05e5-4808-dcd1-ac3f0fc48054"
      },
      "source": [
        "# Perform lemmatization on all of the words in the comments\n",
        "\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "lemmatized = [lemmatizer.lemmatize(comment) for comment in comments if comment.isalpha()]\n",
        "print(lemmatized)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ee585613c629>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlemmatized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcomment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomments\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcomment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-ee585613c629>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlemmatized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcomment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomments\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcomment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'isalpha'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra_fmLQeXzay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}