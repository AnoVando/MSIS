{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnoVando/MSIS/blob/master/MSIS521_IA3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbVNDUFthGZ4",
        "colab_type": "code",
        "outputId": "47ebe1ed-9072-4f7a-dcdd-de3b4b66edf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt') # tokenizer\n",
        "nltk.download('wordnet') # lemmatizer\n",
        "nltk.download('stopwords') # used to handle words like a, an, the\n",
        "nltk.download('averaged_perceptron_tagger') # Part of Speech\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "np.random.seed(2018)\n",
        "\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.metrics import *\n",
        "from sklearn.metrics.pairwise import *\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FnJ3WbafJHs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://github.com/AnoVando/MSIS/raw/master/IA3.csv'\n",
        "data = pd.read_csv(url, header='infer')\n",
        "reviews = data['review'].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8iqJL8LhDxt",
        "colab_type": "code",
        "outputId": "72a2ecd0-8faa-4d05-be6c-c8648c16a920",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "print(data[:5])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   id                                             review       label\n",
            "0   1  About the shop: There is a restaurant in Soi L...  restaurant\n",
            "1   2  About the shop: Through this store for about t...  restaurant\n",
            "2   3  Roast Coffee &amp; Eatery is a restaurant loca...  restaurant\n",
            "3   4  Eat from the children. The shop is opposite. P...  restaurant\n",
            "4   5  The Ak 1 shop at another branch tastes the sam...  restaurant\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-iBJXnZwCfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "custom_list = ['quot', 'ha', 'wa']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IKQu0w_fGFD",
        "colab_type": "text"
      },
      "source": [
        "Part 1. Topic Model\n",
        "There are 1000 reviews for restaurants and films in a collection under the attached csv file. All of those\n",
        "reviews are saved as text files. In this assignment, you are required to investigate the topics of those\n",
        "reviews. In particular, please follow the steps listed below:\n",
        "1. Transform those reviews into a term‐document matrix, lemmatize all the words, remove the\n",
        "stop‐words and punctuations, set the minimal document frequency for each term to be 5 and\n",
        "include 2‐gram.\n",
        "2. Use the LDA model to extract the topics of each document. In particular, we assume there are 6\n",
        "topics.\n",
        "3. Report the topic distribution and the top‐2 topics of the first 10 restaurant reviews (id = [1:10])\n",
        "and the first 10 movie reviews (id = [501:510]).\n",
        "4. Find the top‐5 terms (terms with the top‐5 highest weights) for each of the 6 topics. Based on\n",
        "those terms, describe what those topics are about.\n",
        "5. Based on finding in 3 and 4, describe what review 1 [ID=1] and review 501 [ID=501] are about?\n",
        "Please submit 1 file:\n",
        "A word file includes python code with your comment #, and one screenshot on your Jupyter\n",
        "Notebook showing that your code has run through successfully for each of the first four steps (4\n",
        "screenshots in total). Also, report your answers to question 3, 4, and 5 at the end of the word\n",
        "file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt_sB5dk8F-k",
        "colab_type": "text"
      },
      "source": [
        "1. Transform those reviews into a term‐document matrix, lemmatize all the words, remove the\n",
        "stop‐words and punctuations, set the minimal document frequency for each term to be 5 and\n",
        "include 2‐gram."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfbEn_gdW_lm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmer = PorterStemmer()\n",
        "def lemmatize_stemming(text):\n",
        "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "def preprocess(text):\n",
        "    result = []\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "        if token not in stopwords.words('english'):\n",
        "            if token not in custom_list:\n",
        "                result.append(lemmatize_stemming(token))\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwIp0pTcXWjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processed_docs = data['review'].fillna('').astype(str).map(preprocess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kgu7jVPTZmOU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "c139bac2-8e23-46c0-bbd2-3082203b75f4"
      },
      "source": [
        "print(processed_docs)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0      [shop, restaur, soi, langsuan, road, insid, lu...\n",
            "1      [shop, store, three, year, first, time, tri, r...\n",
            "2      [roast, coffe, amp, eateri, restaur, locat, se...\n",
            "3      [eat, children, shop, opposit, phra, prathat, ...\n",
            "4      [ak, shop, anoth, branch, tast, concentr, tell...\n",
            "                             ...                        \n",
            "995    [peopl, aliv, never, die, difficult, know, go,...\n",
            "996    [first, time, know, chen, tianx, movi, nuclear...\n",
            "997    [film, time, tear, liter, tear, ya, also, leav...\n",
            "998    [rememb, child, teacher, alway, take, troubl, ...\n",
            "999    [abil, episod, mouth, sasuk, year, old, gradua...\n",
            "Name: review, Length: 1000, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4t5uUx3aQe5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
        "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RaPD0J0cfwcj",
        "colab": {}
      },
      "source": [
        "# Generate TF-IDF Vectors\n",
        "#processed_tfidf = [\" \".join(x) for x in processed]\n",
        "\n",
        "#tfidf = TfidfVectorizer(ngram_range=(2, 2), min_df=5) # 2-grams and min. document frequency of 5\n",
        "#tfidf.fit(bow_corpus)\n",
        "#tfidf = tfidf.transform(bow_corpus)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XevlJaMF8O3K",
        "colab_type": "text"
      },
      "source": [
        "2. Use the LDA model to extract the topics of each document. In particular, we assume there are 6\n",
        "topics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irtdaelDvifl",
        "colab_type": "code",
        "outputId": "aa9fae20-8865-4747-b335-ea5946537c88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "lda_model = gensim.models.LdaModel(bow_corpus, num_topics=6, id2word=dictionary)\n",
        "\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic: 0 \n",
            "Words: 0.013*\"film\" + 0.007*\"also\" + 0.006*\"peopl\" + 0.005*\"make\" + 0.005*\"love\" + 0.005*\"stori\" + 0.005*\"time\" + 0.005*\"eat\" + 0.004*\"gt\" + 0.004*\"like\"\n",
            "Topic: 1 \n",
            "Words: 0.012*\"film\" + 0.010*\"like\" + 0.008*\"also\" + 0.007*\"love\" + 0.007*\"peopl\" + 0.007*\"good\" + 0.006*\"make\" + 0.005*\"time\" + 0.004*\"say\" + 0.004*\"think\"\n",
            "Topic: 2 \n",
            "Words: 0.012*\"film\" + 0.007*\"love\" + 0.006*\"peopl\" + 0.006*\"also\" + 0.006*\"time\" + 0.006*\"like\" + 0.005*\"say\" + 0.005*\"movi\" + 0.004*\"one\" + 0.004*\"see\"\n",
            "Topic: 3 \n",
            "Words: 0.009*\"peopl\" + 0.008*\"film\" + 0.007*\"good\" + 0.006*\"also\" + 0.006*\"like\" + 0.006*\"eat\" + 0.006*\"time\" + 0.005*\"movi\" + 0.005*\"love\" + 0.005*\"one\"\n",
            "Topic: 4 \n",
            "Words: 0.011*\"good\" + 0.009*\"like\" + 0.007*\"peopl\" + 0.007*\"time\" + 0.006*\"food\" + 0.006*\"love\" + 0.006*\"eat\" + 0.005*\"say\" + 0.005*\"tast\" + 0.005*\"make\"\n",
            "Topic: 5 \n",
            "Words: 0.012*\"film\" + 0.010*\"peopl\" + 0.007*\"also\" + 0.006*\"good\" + 0.006*\"like\" + 0.006*\"love\" + 0.005*\"say\" + 0.005*\"life\" + 0.005*\"one\" + 0.004*\"time\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdFQZ52O8avy",
        "colab_type": "text"
      },
      "source": [
        "3. Report the topic distribution and the top‐2 topics of the first 10 restaurant reviews (id = [1:10])\n",
        "and the first 10 movie reviews (id = [501:510])."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzrMEeXjWbKn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "aff8c4f4-d35d-4409-c580-de749b22f967"
      },
      "source": [
        "for n in range(10):\n",
        "  print(lda_model[bow_corpus][n])\n",
        "      "
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(4, 0.9940009)]\n",
            "[(3, 0.9445475), (4, 0.05032585)]\n",
            "[(3, 0.19050916), (4, 0.8058597)]\n",
            "[(3, 0.7169321), (4, 0.27477002)]\n",
            "[(3, 0.87687755), (4, 0.10143518)]\n",
            "[(3, 0.47085765), (4, 0.5254309)]\n",
            "[(3, 0.5388849), (4, 0.45329475)]\n",
            "[(4, 0.9892479)]\n",
            "[(0, 0.023919497), (1, 0.023957796), (2, 0.023916451), (3, 0.024019271), (4, 0.8802408), (5, 0.02394621)]\n",
            "[(3, 0.95066565)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypdCZB5fhTZK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "c0af42e2-a99e-45bf-b9eb-a7932113b9bb"
      },
      "source": [
        "for n in range(510):\n",
        "  if n >= 500:\n",
        "      print(lda_model[bow_corpus][n])\n",
        "  if n > 510:\n",
        "      break\n",
        "  n = n + 1"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(1, 0.69372094), (2, 0.3011373)]\n",
            "[(2, 0.9824048), (4, 0.01547226)]\n",
            "[(1, 0.0387394), (2, 0.6812616), (5, 0.27911204)]\n",
            "[(3, 0.9649443)]\n",
            "[(1, 0.4526614), (2, 0.54581934)]\n",
            "[(2, 0.9890299)]\n",
            "[(2, 0.71296626), (5, 0.28189638)]\n",
            "[(2, 0.9752716)]\n",
            "[(0, 0.17565808), (2, 0.7278081), (5, 0.09505909)]\n",
            "[(0, 0.09713469), (1, 0.32730865), (2, 0.2963496), (3, 0.20852555), (5, 0.07020713)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRvtJ9pvZj8y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "0bcf0e38-d4d9-4653-a2bb-308a6d42cfd8"
      },
      "source": [
        "url = 'https://github.com/AnoVando/MSIS/raw/master/IA3.csv'\n",
        "data = pd.read_csv(url, header='infer')\n",
        "data1 = data[0:9]\n",
        "data2 = data[500:509]\n",
        "data3 = data1.append(data2)\n",
        "reviews2 = data3['review'].tolist()\n",
        "\n",
        "# Tokenize, Lemmatize and Remove Stop Words\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "processed2 = []\n",
        "for review in reviews2:\n",
        "    tokens = nltk.word_tokenize(review.lower())\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n",
        "    tokens = [token for token in tokens if not token in stopwords.words('english')]\n",
        "    tokens = [token for token in tokens if not token in STOPWORDS]\n",
        "    tokens = [token for token in tokens if not token in custom_list]\n",
        "    processed2.append(tokens)\n",
        "\n",
        "text = gensim.corpora.Dictionary(processed2)\n",
        "corpus = [text.doc2bow(doc) for doc in processed2]\n",
        "\n",
        "lda_model2 = gensim.models.LdaModel(corpus, num_topics=2, id2word=text)\n",
        "\n",
        "for idx, topic in lda_model2.print_topics(-1):\n",
        "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic: 0 \n",
            "Words: 0.014*\"film\" + 0.007*\"like\" + 0.007*\"good\" + 0.006*\"taste\" + 0.005*\"eat\" + 0.005*\"people\" + 0.005*\"lone\" + 0.005*\"baht\" + 0.005*\"life\" + 0.005*\"delicious\"\n",
            "Topic: 1 \n",
            "Words: 0.009*\"film\" + 0.009*\"like\" + 0.009*\"dolphin\" + 0.008*\"good\" + 0.007*\"people\" + 0.006*\"eat\" + 0.006*\"time\" + 0.006*\"price\" + 0.005*\"restaurant\" + 0.005*\"life\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0pubMMO3NHq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install pyLDAvis if necessary\n",
        "# !pip install pyLDAvis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKhvrHO-M4Nm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize the topics\n",
        "#import pyLDAvis\n",
        "#import pyLDAvis.gensim\n",
        "#pyLDAvis.enable_notebook()\n",
        "#vis = pyLDAvis.gensim.prepare(lda_model2, corpus, text)\n",
        "#vis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQaqHsATf81T",
        "colab_type": "text"
      },
      "source": [
        "4. Find the top‐5 terms (terms with the top‐5 highest weights) for each of the 6 topics. Based on\n",
        "those terms, describe what those topics are about."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQkV8Jxxf5dj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "692a4d17-b95f-40ea-c61c-3a6136c0fd71"
      },
      "source": [
        "lda_model.show_topics(num_topics=6, num_words=5)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.014*\"film\" + 0.010*\"like\" + 0.009*\"good\" + 0.008*\"people\" + 0.006*\"movie\"'),\n",
              " (1,\n",
              "  '0.011*\"film\" + 0.007*\"like\" + 0.007*\"good\" + 0.006*\"time\" + 0.005*\"love\"'),\n",
              " (2,\n",
              "  '0.010*\"film\" + 0.009*\"good\" + 0.008*\"people\" + 0.006*\"love\" + 0.006*\"time\"'),\n",
              " (3,\n",
              "  '0.012*\"film\" + 0.009*\"people\" + 0.008*\"good\" + 0.007*\"love\" + 0.006*\"like\"'),\n",
              " (4,\n",
              "  '0.011*\"people\" + 0.008*\"film\" + 0.007*\"love\" + 0.007*\"time\" + 0.006*\"like\"'),\n",
              " (5,\n",
              "  '0.011*\"film\" + 0.009*\"like\" + 0.008*\"people\" + 0.008*\"time\" + 0.006*\"movie\"')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v72CInnT1e7o",
        "colab_type": "text"
      },
      "source": [
        "Topic 0 is about films and good food.\n",
        "Topic 1 is about people and films about times they love.\n",
        "Topic 2 is about people and films about eating good food.\n",
        "Topic 3 is about people and films about people they like.\n",
        "Topic 4 is about films that people love.\n",
        "Topic 5 is about films about loving people."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJX8H_KjaV7S",
        "colab_type": "text"
      },
      "source": [
        "5. Based on finding in 3 and 4, describe what review 1 [ID=1] and review 501 [ID=501] are about?\n",
        "Please submit 1 file:\n",
        "A word file includes python code with your comment #, and one screenshot on your Jupyter\n",
        "Notebook showing that your code has run through successfully for each of the first four steps (4\n",
        "screenshots in total). Also, report your answers to question 3, 4, and 5 at the end of the word\n",
        "file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVdNQ7Q7urN9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "63bbeb9f-c05a-421b-c7b7-14e46c65dc94"
      },
      "source": [
        "print(lda_model2[corpus[1]])\n",
        "data['review'][0]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, 0.45443976), (1, 0.54556024)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'About the shop: There is a restaurant in Soi Langsuan (Road) inside of Luxx Hotel. The décor of the restaurant: decorated in a rustic style, white walls, glass tables, red chairs, parquet floors, dim lights open at night, the romantic atmosphere: Duck l&#39;orange Pork Wellington and French onion soup. Average Price: 250-450 Baht Food Review: Duck l&#39;orange (455) Duck breast sliced Pork Wellington (445) is a piece of tender pork stuffed with stuffing and wrapped in a thin pastry and then baked to serve with the sauce. Duck roll (285) is a Duck wrapped with vegetables and dough wrap not much delicious French onion soup (235) Sweet taste Garnish with cheese Bake Scallop (225) is a scallop in a thick cream with cheese. Serve with a thin toast to eat together. Score by topic: &lt;Atmosphere&gt; 8/10 Atmosphere nice romantic &lt;food taste&gt; 7/10&lt;Service&gt; 9/10 Good service, good food recommendation &lt;Value&gt; 6/10 price is quite expensive and about 1,000 baht if not ordered the original special. Conclusion: The atmosphere is good, some delicious food. Three and a half stars for some delicious food. Note: This review is for personal use only. We do not like it. If not, please forgive me. I think that is one comment. To win the competition. I have a very good restaurant now. Thank you'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G07YNpsqbffw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6nCKhMTTfs9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m234VGQOOG0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize, Lemmatize and Remove Stop Words\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "processed = []\n",
        "for review in reviews:\n",
        "    tokens = nltk.word_tokenize(review.lower())\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n",
        "    tokens = [token for token in tokens if not token in stopwords.words('english')]\n",
        "    tokens = [token for token in tokens if not token in STOPWORDS]\n",
        "    tokens = [token for token in tokens if not token in custom_list]\n",
        "    processed.append(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}